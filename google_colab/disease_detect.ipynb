{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05v1r1rnwrU_",
        "outputId": "3985f3c7-7abc-46fc-c72d-8fbcda5bc970"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install pillow transformers torch torchvision flask pyngrok==7.1.3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djBBrgmjxHlv",
        "outputId": "5089808d-d9f2-43d3-ca61-7860dc89e3e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ngrok token set\n"
          ]
        }
      ],
      "source": [
        "# <<< REQUIRED if you want a more stable tunnel >>>\n",
        "#add your token\n",
        "NGROK_AUTH_TOKEN = \"\"  # or leave \"\" to run without auth\n",
        "\n",
        "from pyngrok import ngrok\n",
        "if NGROK_AUTH_TOKEN and NGROK_AUTH_TOKEN.strip():\n",
        "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "print(\"ngrok token set\" if NGROK_AUTH_TOKEN else \"no ngrok token provided (shorter sessions)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "9ff8e552eb694bbcb11ff4b370df72ab",
            "8ef45ad9f46f49219cc8a60a34bae598",
            "4a766aaf732f45f0859e51bdedf56a8e"
          ]
        },
        "id": "J1poncF1yBoM",
        "outputId": "3c1b000c-9d51-4acc-8812-93d5f152a949"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ff8e552eb694bbcb11ff4b370df72ab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/408 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/models/mobilenet_v2/feature_extraction_mobilenet_v2.py:30: FutureWarning: The class MobileNetV2FeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use MobileNetV2ImageProcessor instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8ef45ad9f46f49219cc8a60a34bae598",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a766aaf732f45f0859e51bdedf56a8e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/9.34M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "(38, [(0, 'Apple Scab'), (1, 'Apple with Black Rot'), (2, 'Cedar Apple Rust')])"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
        "\n",
        "MODEL_NAME = \"linkanjarad/mobilenet_v2_1.0_224-plant-disease-identification\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "processor = AutoImageProcessor.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForImageClassification.from_pretrained(MODEL_NAME).to(device).eval()\n",
        "\n",
        "# quick sanity check\n",
        "len(model.config.id2label), list(model.config.id2label.items())[:3]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgpU4HPGzEKg"
      },
      "outputs": [],
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from PIL import Image\n",
        "import io\n",
        "import torch.nn.functional as F\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "def predict_image(pil_img, top_k=1):\n",
        "    # preprocess\n",
        "    inputs = processor(images=pil_img, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        probs = F.softmax(logits, dim=-1)[0]\n",
        "\n",
        "    # top-k\n",
        "    k = max(1, min(top_k, logits.shape[-1]))\n",
        "    values, indices = torch.topk(probs, k)\n",
        "    labels = [model.config.id2label[int(i)] for i in indices]\n",
        "    confidences = [float(v) for v in values]\n",
        "    return labels, confidences\n",
        "\n",
        "@app.route(\"/\", methods=[\"GET\"])\n",
        "def health():\n",
        "    return jsonify({\"status\": \"ok\", \"model\": MODEL_NAME})\n",
        "\n",
        "@app.route(\"/predict\", methods=[\"POST\"])\n",
        "def predict_route():\n",
        "    if \"file\" not in request.files:\n",
        "        return jsonify({\"error\": \"No file uploaded. Send multipart/form-data with field name 'file'.\"}), 400\n",
        "    file = request.files[\"file\"]\n",
        "\n",
        "    try:\n",
        "        img = Image.open(io.BytesIO(file.read())).convert(\"RGB\")\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": f\"Invalid image: {e}\"}), 400\n",
        "\n",
        "    # optional: top_k param (default 1)\n",
        "    try:\n",
        "        top_k = int(request.args.get(\"top_k\", 1))\n",
        "    except:\n",
        "        top_k = 1\n",
        "\n",
        "    labels, confidences = predict_image(img, top_k=top_k)\n",
        "    response = {\n",
        "        \"disease\": labels[0],\n",
        "        \"confidence\": round(confidences[0], 4),\n",
        "    }\n",
        "    if top_k > 1:\n",
        "        response[\"top_k\"] = [\n",
        "            {\"label\": l, \"confidence\": round(c, 4)}\n",
        "            for l, c in zip(labels, confidences)\n",
        "        ]\n",
        "    return jsonify(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hfhq4QbDzKku",
        "outputId": "c912a71b-f795-4858-dce3-47bb616d4da9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”— Public URL: NgrokTunnel: \"https://dfd73c635e35.ngrok-free.app\" -> \"http://localhost:5000\"\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Address already in use\n",
            "Port 5000 is in use by another program. Either identify and stop that program, or start the server with a different port.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Flask server started.\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "import threading, time\n",
        "\n",
        "# Make sure no previous tunnels are running\n",
        "try:\n",
        "    ngrok.kill()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "public_url = ngrok.connect(addr=5000, bind_tls=True)\n",
        "print(\"ğŸ”— Public URL:\", public_url)\n",
        "\n",
        "def run_flask():\n",
        "    # host=0.0.0.0 to bind all interfaces inside the Colab VM\n",
        "    app.run(host=\"0.0.0.0\", port=5000, debug=False)\n",
        "\n",
        "thread = threading.Thread(target=run_flask, daemon=True)\n",
        "thread.start()\n",
        "\n",
        "# Small wait so the server starts\n",
        "time.sleep(2)\n",
        "print(\"Flask server started.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
